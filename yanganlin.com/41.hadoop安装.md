---
title: hadoop安装
date: 2018-12-10 23:56:56
categories: 
- Hadoop
tags: 
- 安装
- 大数据
---
学大数据啦,有点小兴奋......  
![](https://image.yanganlin.com//18-12-10/39008359.jpg)
<!-- more -->
# 准备
* 需要jdk的包
* hadoop的包

# 修改配置文件
* 配置文件都在`$HADOOP_HOME/etc/hadoop`
--- 

### 1. 配置hadoop,把`JAVA_HOME`的变量写死
```properties
vim hadoop-env.sh
export JAVA_HOME=/usr/java/jdk1.7.0_65
```
### 2. 编辑`core-site.xml`
```xml
<!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 -->
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://weekend-1206-01:9000</value>
</property>
<!-- 指定hadoop运行时产生文件的存储目录 -->
<property>
	<name>hadoop.tmp.dir</name>
	<value>/home/hadoop/hadoop-2.4.1/tmp</value>
</property>
```

### 3.编辑`hdfs-site.xml`
```xml
<!-- 指定HDFS副本的数量 -->
<property>
	<name>dfs.replication</name>
	<value>1</value>
</property>        	
```

### 4.编辑`mapred-site.xml`
```shell
# 拷贝个文件
mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
```
```xml
<!-- 指定mr运行在yarn上 -->
<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
</property>
```

### 5.编辑`yarn-site.xml`
```xml
<!-- 指定YARN的老大（ResourceManager）的地址 -->
<property>
	<name>yarn.resourcemanager.hostname</name>
	<value>localhost</value>
</property>
<!-- reducer获取数据的方式 -->
<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
</property>
```

### 6.将hadoop添加到环境变量
```shell
export HADOOP_HOME=/itcast/hadoop-2.4.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source /etc/profile
```

### 7.格式化namenode（是对namenode进行初始化）
```shell
hadoop namenode -format
```

### 8.启动hadoop
```shell
先启动HDFS
sbin/start-dfs.sh

再启动YARN
sbin/start-yarn.sh
```

### 9.验证是否成功
```shell
使用jps命令验证
# 这两个忘记是属于谁了
27408 NameNode
28218 Jps
# 下面四个是属于hadoop
27643 SecondaryNameNode
28066 NodeManager
27803 ResourceManager
27512 DataNode
```

访问
```java
http://192.168.1.101:50070 （HDFS管理界面）
http://192.168.1.101:8088 （MR管理界面）
```

### 10.测试着玩下
```shell
# 上传一个文件
hadoop fs -put jdk-8u191-linux-x64.tar.gz hdfs://weekend110:9000/

# 获取一个文件
hdfs://weekend110:9000/
hadoop fs -get  hdfs://weekend110:9000/文件名

# 查看数据
hadoop fs -cat /wordcount/output/part-r-00000

# 运行一个程序,是找input文件夹下的文本,并且分词去计算单词有多少个
hadoop jar hadoop-mapreduce-examples-2.4.1.jar wordcount /wordcount/input /wordcount/output
```